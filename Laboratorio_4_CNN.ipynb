{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 4 - Aprendizaje profundo para la clasificación de imágenes\n",
    "\n",
    "### Roberto Castillo\n",
    "### Juan Lorthiois\n",
    "\n",
    "Siga las instrucciones en negritas para completar el laboratorio.\n",
    "\n",
    "------------\n",
    "\n",
    "## El reto\n",
    "\n",
    "**Su tarea es la de construir un clasificador de imágenes usando Keras (Tensorflow) y Redes Neuronales Convolucionales (CNN) para un conjunto de datos conocido como \"Fashion MNIST dataset\"\"\n",
    ". Este conjunto de datos incluye 10 etiquetas de diferentes tipos de ropa con imágenes de 28 by 28 *escalagris*. Hay un conjunto de datos de entrenamiento de 60,000 imágenes y un conjunto de datos de prueba de 10,000 imágenes.**\n",
    "\n",
    "    Etiqueta\tDescripción\n",
    "    0\t    T-shirt/top\n",
    "    1\t    Trouser\n",
    "    2\t    Pullover\n",
    "    3\t    Dress\n",
    "    4\t    Coat\n",
    "    5\t    Sandal\n",
    "    6\t    Shirt\n",
    "    7\t    Sneaker\n",
    "    8\t    Bag\n",
    "    9\t    Ankle boot\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos\n",
    "\n",
    "**Tarea 1: Ejecute el siguiente código para descargar los datos usando Keras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlort\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(X_entreno, y_entreno), (X_prueba, y_prueba) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los Datos\n",
    "\n",
    "**TAREA 2: Utilice matplotlib para visualizar una imagen del conjunto de datos.  Puede ser cualquier imagen del conjunto de datos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.randint(0, 60000)\n",
    "image = X_entreno[sample]\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los Datos\n",
    "\n",
    "**TAREA 3: Normalice los datos X entreno y X prueba dividiendo por el valor máximo de los arreglos de las imágenes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "print(X_entreno.max())\n",
    "print(X_prueba.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entreno= X_entreno / 255\n",
    "X_prueba= X_prueba /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_entreno.max())\n",
    "print(X_prueba.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAREA 4: Cambie el formato de los arreglos X para que incluyan una 4rta dimensión del canal de color. Similar a lo que se hizo en clase para el conjunto de datos MNIST de números.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_entreno.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entreno = X_entreno.reshape(60000, 28, 28, 1)\n",
    "X_prueba = X_prueba.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAREA 5: Convierta los valores de y_entreno y y_prueba para que estén \"one-hot encoded\" para poder hacer un análisis categórico con Keras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categorica_entreno = to_categorical(y_entreno)\n",
    "y_categorica_prueba = to_categorical(y_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del Modelo\n",
    "\n",
    "**TAREA 6: Utilice Keras para crear un modelo que contenga, al menos, las siguientes capas (pero siéntase en libertad de experimentar):**\n",
    "\n",
    "* Capa \"2D Convolutional\", filtros = 32 y tamaño_kernel = (4, 4)\n",
    "* Capa de \"Pooling\"\"\n",
    " de tamaño = (2, 2)\n",
    "\n",
    "* Capa de Aplanado\n",
    "* Capa Densa (128 unidades, pero siéntase en libertad de \"jugar\"con este valor), activación RELU\n",
    "\n",
    "* Una capa Final Densa de 10 unidades con activación softmax\n",
    "\n",
    "**Luego compile el modelo con estos parámetros: loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy']**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos las librerias necesarias (vistas en clase)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "modelo = Sequential()\n",
    "\n",
    "#Agregamos la capa \"2dConvolucional\" con 32 filtros y un tamaño de kernel (4,4)\n",
    "modelo.add(Conv2D(filters = 32, kernel_size = (4, 4), input_shape = (28,28,1)))\n",
    "\n",
    "#Agregamos la capa de Pooling de tamaño (2,2)\n",
    "modelo.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "#Capa de aplanado\n",
    "modelo.add(Flatten())\n",
    "\n",
    "#capa densa de 128 unidades\n",
    "modelo.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "#Capa final densa de 10 unidades\n",
    "modelo.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "#Compilamos el modelo\n",
    "modelo.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 25, 25, 32)        544       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos un resumen de los parametros selccionados anteriormente\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del Modelo\n",
    "**TAREA 6: Entrene/Ajuste el modelo con el conjunto X_entreno set. La cantidad de épocas le queda a Ud determinar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.3949 - accuracy: 0.8584 - val_loss: 0.3223 - val_accuracy: 0.8846\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2712 - accuracy: 0.9017 - val_loss: 0.3043 - val_accuracy: 0.8909\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2256 - accuracy: 0.9178 - val_loss: 0.3058 - val_accuracy: 0.8900\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1931 - accuracy: 0.9312 - val_loss: 0.3032 - val_accuracy: 0.8975\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1679 - accuracy: 0.9401 - val_loss: 0.3125 - val_accuracy: 0.9027\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1469 - accuracy: 0.9479 - val_loss: 0.3188 - val_accuracy: 0.8968\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1279 - accuracy: 0.9557 - val_loss: 0.3384 - val_accuracy: 0.9034\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1126 - accuracy: 0.9595 - val_loss: 0.3831 - val_accuracy: 0.8966\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0996 - accuracy: 0.9648 - val_loss: 0.4361 - val_accuracy: 0.8934\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0898 - accuracy: 0.9686 - val_loss: 0.4717 - val_accuracy: 0.9015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b01490e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ajustamos/entrenamos el modelo a los datos\n",
    "modelo.fit(X_entreno, y_categorica_entreno, epochs = 10, validation_data = (X_prueba, y_categorica_prueba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo\n",
    "\n",
    "**TAREA 7: Muestre los valores de [accuracy, precision, recall, f1-score] que logró el modelo con el conjunto de datos X_prueba data set. Tenga en mente que hay múltiples formas de hacer esto.  Sin embargo, le recomendamos que utilice el mismo procedimiento usado mencionado en la parte de intuición, en clase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos que contamos con las metricas usuales de accuracy y perdida\n",
    "modelo.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias necesarias para evaluar el modelo\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos predicciones sobre el conjunto de prueba (el formato es un poco distinto al visto en clase por la version de tensorflow empleada)\n",
    "predicciones = modelo.predict(X_prueba) \n",
    "\n",
    "#Obtenemos los datos categoricos\n",
    "clases=np.argmax(predicciones,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84      1000\n",
      "           1       1.00      0.96      0.98      1000\n",
      "           2       0.87      0.83      0.85      1000\n",
      "           3       0.88      0.93      0.90      1000\n",
      "           4       0.83      0.87      0.85      1000\n",
      "           5       0.99      0.95      0.97      1000\n",
      "           6       0.70      0.78      0.74      1000\n",
      "           7       0.93      0.98      0.95      1000\n",
      "           8       0.98      0.97      0.98      1000\n",
      "           9       0.96      0.96      0.96      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generamos el reporte\n",
    "print(classification_report(y_prueba, clases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos ver que el f1 score es superior a 0.96 en la utlima iteracion lo cual es considerado \"excelente\"\n",
    "\n",
    "### Además, el accuracy al comparar las predicciones con los datos de prueba resulta ser del 0.9 lo cual también es excelente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gran trabajo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
